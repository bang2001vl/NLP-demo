# -*- coding: utf-8 -*-
from __future__ import print_function
from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
from pyvi import ViTokenizer, ViPosTagger
from joblib import dump
import re
import string
import codecs
import json

# T·ª´ ƒëi·ªÉn t√≠ch c·ª±c, ti√™u c·ª±c, ph·ªß ƒë·ªãnh
path_pos = "../input/customdata/pos.txt"
path_nag = "../input/customdata/nag.txt"
path_not = "../input/customdata/phu_dinh.txt"
path_replace_list = "../input/customdata/replace-list.json"
# File dataset
path_native_train = "../input/customdata/train.txt"
path_add_train1 = "../input/customdata/train_nor_811-format.txt"
path_add_train2 = "../input/customdata/test_nor_811-format.txt"

nag_list = []
with codecs.open(path_nag, "r", encoding="UTF-8") as f:
    for n in f.readlines():
        nag_list.append(n.replace("\n", "").replace(" ", "_"))

pos_list = []
with codecs.open(path_pos, "r", encoding="UTF-8") as f:
    for n in f.readlines():
        pos_list.append(n.replace("\n", "").replace(" ", "_"))

not_list = []
with codecs.open(path_not, "r", encoding="UTF-8") as f:
    for n in f.readlines():
        not_list.append(n.replace("\n", "").replace(" ", "_"))

with codecs.open(path_replace_list, "r", encoding="UTF-8") as f:
    replace_json = json.load(f)

# H√†m chuy·ªÉn th√†nh c√¢u kh√¥ng d·∫•u
def no_marks(s):
    VN_CHARS_LOWER = "·∫°·∫£√£√†√°√¢·∫≠·∫ß·∫•·∫©·∫´ƒÉ·∫Ø·∫±·∫∑·∫≥·∫µ√≥√≤·ªç√µ·ªè√¥·ªô·ªï·ªó·ªì·ªë∆°·ªù·ªõ·ª£·ªü·ª°√©√®·∫ª·∫π·∫Ω√™·∫ø·ªÅ·ªá·ªÉ·ªÖ√∫√π·ª•·ªß≈©∆∞·ª±·ªØ·ª≠·ª´·ª©√≠√¨·ªã·ªâƒ©√Ω·ª≥·ª∑·ªµ·ªπƒë√∞"
    VN_CHARS_UPPER = "·∫†·∫¢√É√Ä√Å√Ç·∫¨·∫¶·∫§·∫®·∫™ƒÇ·∫Æ·∫∞·∫∂·∫≤·∫¥√ì√í·ªå√ï·ªé√î·ªò·ªî·ªñ·ªí·ªê∆†·ªú·ªö·ª¢·ªû·ª†√â√à·∫∫·∫∏·∫º√ä·∫æ·ªÄ·ªÜ·ªÇ·ªÑ√ö√ô·ª§·ª¶≈®∆Ø·ª∞·ªÆ·ª¨·ª™·ª®√ç√å·ªä·ªàƒ®√ù·ª≤·ª∂·ª¥·ª∏√êƒê"
    VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER
    __INTAB = [ch for ch in VN_CHARS]
    __OUTTAB = "a" * 17 + "o" * 17 + "e" * 11 + "u" * 11 + "i" * 5 + "y" * 5 + "d" * 2
    __OUTTAB += "A" * 17 + "O" * 17 + "E" * 11 + "U" * 11 + "I" * 5 + "Y" * 5 + "D" * 2
    __r = re.compile("|".join(__INTAB))
    __replaces_dict = dict(zip(__INTAB, __OUTTAB))
    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)
    return result

# H√†m ti·ªÅn x·ª≠ l√≠ text
def normalize_text(text):
    # Remove c√°c k√Ω t·ª± k√©o d√†i: vd: ƒë·∫πppppppp -> ƒë·∫πp
    text = re.sub(
        r"([A-Z])\1+", lambda m: m.group(1).upper(), text, flags=re.IGNORECASE
    )
    
    # Remove c√°c kho·∫£ng tr·∫Øng l·∫∑p l·∫°i: vd: "a   a" => "a a"
    text = re.sub(' +', ' ', text)
    text = text.strip()

    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    text = text.lower()

    # Thay m·ªôt s·ªë t·ª´ l√≥ng, t·ª´ vi·∫øt t·∫Øt, icons, ti·∫øng anh th√†nh t·ª´ c√≥ nghƒ©a
    for key in replace_json:
        text = text.replace(key, replace_json[key])

    # Chuy·ªÉn d·∫•u c√¢u th√†nh space vd: a,b.c -> a b c
    translator = str.maketrans(string.punctuation, " " * len(string.punctuation))
    text = text.translate(translator)

    # Chia c√¢u th√†nh c√°c token vd: "Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi" -> ['Tr∆∞·ªùng', 'ƒë·∫°i_h·ªçc', 'b√°ch_khoa', 'h√†_n·ªôi']
    tokens, tags = ViPosTagger.postagging(ViTokenizer.tokenize(text))

    # Gom c√¢u l·∫°i
    text = " ".join(tokens)
    
    # remove n·ªët nh·ªØng k√Ω t·ª± th·ª´a th√£i
    text = text.replace('"', " ")
    text = text.replace("Ô∏è", "")
    text = text.replace("üèª", "")

    return text

# L·∫•y d·ªØ li·ªáu t·ª´ file train
def load_data_from_file(filename, is_train=True):
    lst = []
    sample = []

    with open(filename, "r") as file:
        for line in file:
            if (line == "\n") and (len(sample) > 2) and (sample[-1].isnumeric()):
                lst.append([sample[0], "  ".join(sample[1:-1]), sample[-1]])
                sample = []
            else:
                sample.append(line.replace("\n", ""))
    return lst

# Dup
def prepare_data_set(x_set, y_set):
    X, y = [], []
    for document, topic in zip(list(x_set), list(y_set)):
        # Th√™m d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a
        document = normalize_text(document)
        X.append(document.strip())
        y.append(topic)
        # Th√™m d·ªØ li·ªáu ƒë√£ b·ªè d·∫•u
        X.append(no_marks(document))
        y.append(topic)
    return X, y

# Lu·ªìng ch·∫°y ch√≠nh
steps = []
steps.append(
    (
        "CountVectorizer",
        CountVectorizer(ngram_range=(1, 5), stop_words=("r·∫±ng", "th√¨", "l√†", "m√†"), analyzer=str.split),
    )
)
steps.append(
    (
        "tfidf",
        TfidfTransformer(use_idf=False, sublinear_tf=True),
    )
)
steps.append(("classifier", LogisticRegression(max_iter=500)))
# steps.append(("classifier", LinearSVC(max_iter=7000)))
clf = Pipeline(steps)

for epo in range(3):
    data_columns = list(["id", "comment", "label"])

    # D·ªØ li·ªáu native
    train_data = pd.DataFrame(load_data_from_file(path_native_train), columns=data_columns)
    print(train_data)

    # D·ªØ li·ªáu comment 1
    train_data = pd.concat(
        [
            train_data,
            pd.DataFrame(load_data_from_file(path_add_train1), columns=data_columns),
        ]
    ).reset_index(drop=True)
    print(train_data)

    # D·ªØ li·ªáu comment 2
    train_data = pd.concat(
        [
            train_data,
            pd.DataFrame(load_data_from_file(path_add_train2), columns=data_columns),
        ]
    ).reset_index(drop=True)
    print(train_data)

    # train_data = pd.DataFrame([
    #     ["1", "0", "Chi·∫øn d·ªãch r·∫•t √Ω nghƒ©a"],
    #     ["2", "1", "Chi·∫øn d·ªãch kh√¥ng x·ª©ng ƒë√°ng ƒë·ªÉ tham gia"],
    #     ["3", "1", "Chi·∫øn d·ªãch kh√¥ng ƒë√°ng ƒë·ªÉ tham gia"],
    #     ["4", "0", "Chi·∫øn d·ªãch r·∫•t r·∫•t √Ω nghƒ©a"],
    # ],columns=list(['id','label','comment']))

    X_train, X_test, y_train, y_test = train_test_split(
        train_data.comment, train_data.label, test_size=0.3
    )

    # Th√™m m·∫´u b·∫±ng c√°ch l·∫•y trong t·ª´ ƒëi·ªÉn Sentiment (nag/pos)
    dictionary_data = []
    for index, word in enumerate(pos_list):
        dictionary_data.append(["pos" + str(index), word, "0"])

    for index, word in enumerate(nag_list):
        dictionary_data.append(["nag" + str(index), word, "1"])

    for index1, word1 in enumerate(not_list):
        for index2, word2 in enumerate(pos_list):
            dictionary_data.append(
                ["notpos" + str(index1) + "_" + str(index2), word1 + " " + word2, "1"]
            )
        for index2, word2 in enumerate(nag_list):
            dictionary_data.append(
                ["notnag" + str(index1) + "_" + str(index2), word1 + " " + word2, "0"]
            )

    # Th√™m d·ªØ li·ªáu v√†o t·∫≠p train
    dictionary_frame = pd.DataFrame(dictionary_data, columns=data_columns)
    X_train = X_train.append(dictionary_frame.comment)
    y_train = y_train.append(dictionary_frame.label)
    print(len(X_train))

    X_train, y_train = prepare_data_set(X_train, y_train)
    X_test, y_test = prepare_data_set(X_test, y_test)

    print("Starting: Fit model")
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    report1 = metrics.classification_report(y_test, y_pred, labels=['1', '0'], digits=3)

    # #CROSS VALIDATION
    cross_score = cross_val_score(clf, X_train, y_train, cv=5)

    # #REPORT
    print('EPOCH = ' + str(epo))
    print("DATASET LEN %d" % (len(X_train)))
    print("TRAIN 70/30 \n\n", report1)
    print(u'-'*100)

# #TRAIN 100% (Hu·∫•n luy·ªán v·ªõi to√†n b·ªô d·ªØ li·ªáu)
train_data = pd.concat([train_data, dictionary_frame]).reset_index(drop=True)

X_train, y_train = prepare_data_set(train_data.comment, train_data.label)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_train)
report2 = metrics.classification_report(y_train, y_pred, labels=['1', '0'], digits=3)

# #REPORT
print('EPOCH = ' + str(epo))
print("DATASET LEN %d" % (len(X_train)))
print("TRAIN 100% \n\n", report2)
print(
    "CROSSVALIDATION 5 FOLDS: %0.4f (+/- %0.4f)"
    % (cross_score.mean(), cross_score.std() * 2)
)

# # #SAVE MODEL FILE
print("Save model")
dump(clf, "./model.joblib")
